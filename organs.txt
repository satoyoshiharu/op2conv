#操作から会話へ https://satoyoshiharu.github.io/docs/organs.html #HMI #UX #UI #VUI #音声
コンピュータのインターフェイスは、ヒトが道具を操作するという関係から、道具と会話するという関係へ、変わるべきである。
コンピュータは、第二次世界大戦後に、エリートが脳を拡張する道具として始まり発展してきた。しかし、今や、万人がスマホという形態でコンピュータを利用するようになった。その変化にユーザーインターフェイスが追い付いていない。
人は、手指と目以外にも素晴らしい生体機能を持っている。
現在のコンピュータは、特有の抽象概念を利用者に押し付けて、認知負荷をかけている。
ヒトが、しゃべりや身体で意図を表現し、機械は、耳と目を持ち人に反応する、そう変われば、だれでも生得の能力で自然にICTとインターネットを利用できるようになる。
操作から会話へ変化するための技術はすでにあり、ただ、ソフトウェアやアプリのデザインを変えさえすればいい。
石黒浩氏、「人間は技術によって能力を拡張できる。遺伝子の進化よりも技術による能力拡張の方がはるかに速い。ロケットに乗って月に行くことはできるが、遺伝子を改良して月に行けるのはいつの日になるか分からない」
ヒトは、ハサミを難なく使える。それに対して、コンピュータは、マニュアルを読まないと使えない。つまり、コンピュータがもたらす概念体系は、ヒトの頭に素直に自然と入ってくるもので、ない。
ヒトは、意識せずに歩くことができる。また、自転車に乗ったり、自動車のハンドルを操作するのに、少し訓練すれば、あとは無意識的に筋肉が動いてくれる。一方、コンピュータは、少し慣れたあとでも、操作する際、これをやれば何が起きるかと、常に知的な注意と緊張を強いられる。
子供から成長するにつれて、ほとんどのヒトは言葉をしゃべり、文字を書くことができるようになる。これは、社会的教育による。一方、コンピュータは、情報を流通するツールとしても、誰でも使えるものでない。つまり、人が社会の教育的環境の中で自然と身に着けられる、ヒト文化の一部では、ない。
現状のコンピュータは、ヒトの日常的な能力で即、使えるものでない。その意味で、コンピュータはヒトの能力と調和していない。
コンピュータは、エリートの知能の拡張のための道具であった。石器は、ヒトの手の延長だったが、コンピュータは、ヒトの脳の延長だった。
ヒトが道具を操作するのは単方向の関係である。逆に、こちら側のヒトの意図と無関係に、向こう側から来た広告や不要な通知などに、突き動かされる。道具がこちら側のヒトを操作しているかのようである。
会話モデルでは、意図を伝えて、その反応が返ってきて、やり取りを続ける。双方向であることが本質である。意図から反応までの過程は、操作と同じくブラックボックスであるが、反応が意図に沿ったものかそうでないかを、ヒトは容易にわかる。会話は、ヒトが制御しやすい。
従来、ヒトとコンピュータは、ヒトがモノ道具を操作するという関係性だった。また、コンピュータはエリートの知能の拡張のための道具だった。そのため、操作する時に認知負荷があっても、エリートには問題なかった。
コンピュータが、これまでの道具と異なる、一つの決定的な違いは、ヒトの言葉を受け入れられることである。複雑な言葉を使うことは、ヒトの特性と考えられる。機械が、言葉を理解できるかのようにふるまうとしたら、それは、それまでの道具に比べて、質的に決定的な変化だ。機械が、ヒトのパートナーとして、歩み寄ってきている。この点を、もっと利用することで、会話インターフェイスに近づける。
ひと同志は、身体全部を使って、会話する。同様に、意図レベルでやり取りをする際、ヒトのパートナーたる機械道具は、ヒトの身体を相手にしなければならなくなる。なぜなら、ヒトの意図は、ヒトの身体で表現されるからである。
ヒトの意図は、キー操作よりも音声発話のほうが適している。また、注目を示すまなざし、指差し、肯定・否定の首ジェスチャー、手・腕ジェスチャーなども意図を表現する。これからのコンピュータは、このようなヒトの身体とやり取りをするべきである。
運搬ロボットにとって、ヒトがスマホのモニターとタッチから出した指示だけで動いていいのだろうか？　運搬ロボットは、ヒトの歩く通路をヒトと共有する。ロボットは、ヒトの動きを観察しながら動かないと、ぶつかったり危険をもたらしたりする。機械道具側から見ても、ヒトの脳だけでなく、カラダを相手にやり取りをするのが自然であり、それで初めて目的を達成できることが多い。
機械道具はヒトの環境の一部である。それと同時に、ヒトの能力の延長でもある。機械道具は、ヒトと環境の接点に居る。機械道具がヒトと触れるとき、自然な状態では、ヒトはカラダ全体を使い環境と会話する、ということを忘れてはいけない。
ヒトが、認知負荷の高いコンピュータに合わせるのでなく、コンピュータのほうヒトに合わせる。ヒトが意図したことに、コンピュータが答える関係になる。ヒトの意図を軸にやり取りが回るので、ヒトが道具から操作されるということは起きにくい。そのとき、コンピュータはヒトと調和する。
コンピュータの創設期に、知能の拡大を目標としたことは、適切ではなかったかもしれない。最初に、脳の道具という異質なブツというとらえ方よりも、ヒトのパートナーだという発想をしたほうがよかったのかもしれない。
ロボットを専門とする石黒浩氏は、「人間は人間を認識する脳を持っている。人間にとって最も関わりやすいインタフェースは、人間に似たロボットになる」という。コンピュータは、インターフェイスを軽視して、発展してきた。
ヒト同士の会話を、インターフェイスの理想だとする考えは、昔からあった。その実現のためには、コンピュータを作る人、アプリを作る人が、すでにある技術をもとに、ちょっと視点を変えてデザインをすればよい。少なくとも、技術はあって、組み上げ方を変えればよい。
石井裕のタンジブル・ビッツは、豊かなコンピューター・ネットワーク空間の世界を、モニターとマウス越しでしかインターフェイスしないことに異を唱え、環境自体をインターフェイスにしようとした。環境ごと変わる必要はない。コンピュータが変われば済む。			
音声認識技術は、すでにある。言葉を理解しているかのようにコンピュータを動作させる技術は、深層学習による自然言語処理として、すでにある。特定の問題解決タスクや、特定の語彙空間への応用の広がりが、まだないだけである。
音声で対話するボイス・ユーザー・インターフェイスのデザインに関しても、またビジュアルとボイスを融合させたデザインに関しても、経験とノウハウが、まだ少ないだけである。
多くのＩＴ技術者が、インターフェイスの現状はおかしいと、見方を変えればいいのである。その気になればいいのである。
コンピュータやロボットは、ヒトが選別するという環境で、淘汰されて発展していく。遺伝子の世界で、生物の目が決定的な役割をしたのだから、ヒトの作る道具たちが目を持つということは、やはり、将来から見て決定的なことになっていたとしてもおかしくない。
文字の登場時期は、ヒトの進化史から見ると、かなり遅い、最近のことである。ヒトは、ほとんどの時期を、音響言語で過ごしてきた。ヒトにとって、音響言語のほうが、自然でなじみのある表現である。
文字のディジタル表現があるために、テキストのコピーや通信や、計算処理は容易だった。文字がなければ、プログラムという概念もありえなかった。
コンピュータは、その創成期には、『知能の拡張』が目標だった。単純だが膨大な弾道軌跡計算を代わりにやってくれた道具の延長である。頭脳労働を助けてくれる、知能を拡張する道具という発想である。ここで、コンピュータの主な利用者は、科学者や研究者という、インテリないし『エリート』だった。
アラン・ケイは、子供でも誰でも使えるコンピュータを狙っていた。つまり、コンピュータの歴史の中で、初めて、想定利用者層の転換という画期的なことをやった。
エリートの知能拡張のための道具としての概念立てということに対し、エリートたちに批判的な思考は起きなかった。ファイル、ウィンドウ、アイコン、メニュー、ツールバー、カーソルなど、コンピュータ特有の抽象的な概念とそれを実装するコードが、どんどん積み上げられていった。
ドナルド・ノーマンは、行為は七段階の手順を踏むと：　(1) ゴール(2) 実行意図(3) 行為系列展開 (4) 行為系列実行=>外界=> (5) 知覚 (6) 解釈 (7) 評価 => (1)。コンピュータを操作する場合、（３）～（６)が、コンピュータに固有の抽象概念である。
目的とは何か。ドリルを買う人は、ドリルが欲しいのではなく、穴が欲しい。PC が欲しいのは、PC自体が欲しいのではなく、きれいな文章が欲しいから。またきれいなグラフを見たいから。スマホが欲しいのは、いつでもどこでも情報を検索したいから。
コンピュータの操作は、目的達成のための中間の手順をユーザに強いる。中間的な手順は、コンピュータ特有の抽象的概念である。コンピュータは、道具として操作するのに認知負荷があり、理想ではない。
ヒトは、自然な状態では、そもそも、いろんな器官を総合して環境とやり取りをしている。また、文字を扱うよりも、ずっと長い間、音響的言語で生きてきた。目だけで環境を知覚し、手指だけで反応するというGUIは、むしろ特殊な状況ともいえる。
コンピュータは、エリートが知能を拡張するための道具として開発されてきた。エリートたちは、目と手指だけのインターフェイスで、抽象的な概念体系を操作することを、問題視しなかった。
石井裕は、マウスとモニターだけで、人がコンピューター・ネットワーク空間に触れないのは、情けないと言った。そういった道具を使うヒトの能力の面から見ると、目と手指だけでコンピューター・ネットワーク空間という概念体系とやり取りすることが、情けない。
ヒトの大脳皮質の2/3は、諸情報を総合する連合野である。ヒトは感覚や運動を特定の部分に頼って生活してはおらず、総合して環境に対処している。しかし、現在のコンピュータ道具相手には、目と手指だけを使う。それは、不自然に限定されたものである。
ヒトは、視聴覚を融合して言語を用いて考える。しかし、コンピュータを操作しているとき、もっぱら視覚的言語だけを操り、音響器官は遊んでいてもったいない。
手指は、ヒトの体によって器用さを得て、多様な器用さを発揮できる。しかし、コンピュータインターフェイスでは、手の一部の機能、たたく、しか利用していない。コンピュータを操作するとき、箸を操り、鋏を操る手の多彩な能力は、どこへ消えたか？
ヒトは三次元の住民である。しかし、二次元の光る画面をにらんで、コンピュータを操作する。二次元を経由するのは、効率的で、そこで高度な認知作業はできる。が、リアルな世界の三次元の豊かさを生かしていない。
歩いているときに、地図をみたくなった。歩きながら、手でスマホを持ち、眼はスマホにくぎ付けになる。歩いたり転ばないようにバランスをとる知能と、コンピュータを操作する知性は、何の関わりもなく動いている。
視覚は、遠隔感覚である。一方、手指の機械動作系は、近接作用器官である。ヒトが道具を利用するとき、目と手を使う。手の届く範囲で、目を使う。遠くを見る目は利用されていない。
ドナルド・ノーマンによると、日常生活の道具は、物理的な制約、論理的な制約、社会・文化的な制約、意味的な制約などによって、操作できることが絞り込まれている。コンピュータのインターフェイスでは、これらのうち、意味的・論理的な制約くらいしか、利用できない。
グラフィカル・インターフェイスの表示は、メニューとともにアイコンを多用する。アイコンは、視覚的なしるしである。ところが、視覚的なしるしは間接的で表示能力に限界がある。
高齢者は「？」からHELPを連想できない。三点リーダーをタップすると画面に掲載できなかった「コマンド」が出てくるとは想像できない。プラスマークや三角マークをタップすると、何やら展開して新しい詳しい情報が登場してくるとは、想像できない。
PCで、利用者がやりたいのは、きれいな文書を作ることである。そのファイルを、上書き保存や、名前を付けて保存、というのは、なにか？保存する場所に、マイドキュメント、デスクトップ、OneDrive とか出てくるが、どういうことか？利用者にとっては、名前とか保存方法や保存場所などは、どうでもいい。後で、作った文書が取り出せさえすればよい。後で印刷できれば良い。
石井裕は、「いまのUIは抽象化しすぎである」と言った。彼は、そこから、情報という見えない存在を、環境の具体的なものを通して表現する、というタンジブルビッツという考えに至った。しかし抽象化された情報を見たい？抽象的な内部概念は隠してよ。
項目が3個か4個までであれば、それを記憶したり選択するのは、負荷が小さい。ところが、今のグラフィカル・インターフェイスのメニューは、機能を増やして詰め込む、いい掃きだめになっている。メニュー、つまりあるアプリでできることに関して、デザイナーは利用者に必要なことだけに絞り込むことに、多くの場合、失敗している。
スマホが登場したとき、当然、グラフィカル・インターフェイスが応用された。広い画面のインターフェイスが、小さい画面のものへと変化し、マウスはタッチという直接操作へ変わった。しかし、グラフィカル・インターフェイスをもとに、その進化を重ねたため、複雑さはそのまま、ないし、より増した。操作と意味の対応関係が複雑怪奇となった。
ヒトが道具を操作するのは一方向の関係である。そこで、こちらの操作から、向こうからの反応を見るまでは、ブラックボックスである。そこに、逆の関係が入り込む。ヒトが道具に操作されていると解釈もできる現象が、すでに現実に、どこにでも、起きてしまっている。
スマホに通知が無秩序に飛び込んでくる。高齢者であると、通知のマークを消すやり方がわからず、イライラする。そもそも、道具のほうが勝手にヒトに何かをプッシュするというのは、ヒトの意図に反することである。
広告ビジネスモデルは、機械がヒトを操作することを許容している。
ヒトは、身体のしぐさの一つとして、音声で言語表現する。それは、日常的な用語で、意図を表現する。
ヒトの生得的な身体能力を、あるがまま、インターフェイスにする。
コンピュータは、すでに、ヒトに歩み寄り、音のままの言葉を理解し始めている。従来の道具とは、質的に異なる性質である。
音声言語は日常生活空間で使われるため、意図を自然に表現する。話し言葉で、一見、曖昧と見える部分は、話す相手や周囲の状況などのコンテキストで、実は明確である。ヒトの意図は、周りのコンテキストがあれば、言語音声に曖昧性なく自然に表現されている。
音声のみだと、音声をコンピュータに聞かせている状態なのか、そうでないのかの区別ができない。また、コマンドなのかテキストなのかの区別ができない。音声のみでは、これらの制御がやりにくい。「OK Google」とか、「Hey Siri」とか言って、音声をコンピュータに聞き取らせるのを始めるは、不格好である。
音声のみでは、そもそも空間的な位置指定ができない。ヒトは、指差しや、マウスやで、空間の位置指定を容易に行うことができる。しかし、それを音声でやるとなると、あいまいな指示か、かなり冗長な指示しか表現できない。
音声は、ボリューム調整など、アナログ量の制御が苦手である。アナログ量は、空間的な概念である。指つまみで簡単に指定できるボリューム量などは、音声言語で制御するのは難しい。位置指定と同様、空間的な情報は空間的な手段で扱ったほうが良い。
視覚は複数個の並列処理ができ、構造を記憶できる。一方、聴覚は逐次情報を対象とし、一度にたくさんことを相手にすることは苦手である。構造は複数の要素と関係性からなる。構造的な情報を扱うのは、音声＋聴覚は苦手である。
2、3個以上の選択肢がある場合、視覚的な補助なしに音声だけで選択を行うことは困難である。
音声は意図を表現する。その水準でアプリを組もうとしたら、実は、指によるコントロール・空間指示と、視覚による構造把握と、併用せざるを得ない。視聴覚融合は、必然である。
音声によるインターフェイスは、グラフィカル・インターフェイスと同様に、メニュー(ただし4個までね)や画面遷移や問い返しという、視覚やコンテキストによる支持と制約を活用すればよい。
音声アプリの組み方で、有利になる点もある。1回限りの操作に比べて、やり取りが続く会話では、ヒトに、次に何が起きるかという期待と予測が生じる。これが、アプリを組むときに、利用できる制約に追加される。
ヒトのいる３Ｄ空間内位置は、制約として利用できる。例えば、それによって、「あっち」の意味が確定する。
音声表現は、アイデアを素早く完全な形で記録することができる。たとえ誤認識でノイズが多くても、文であるため、後で編集するときに、想起しやすい。その結果、音声でスケッチした後の文書作成はやりやすい。
ヒトは、視線で注目する。ヒトは、指さしで、注目対象を示したりする。注目行動は、会話の中で、意図を解釈するときの重要な制約となる。
ヒトとリアルな世界を一緒にとらえると、アプリが利用できる制約が増える。
スマホ、モニター付きスマートスピーカー、サービスロボット、会話ロボットあるいはAIロボットと呼ばれるロボットなどは、目と耳を持つ。ヒトと機械が会話をするための、ハードパーツそろっている。また、コンピュータは、ヒトの声を聴き、言葉を理解始めた。それらは、ヒトのしゃべりとジェスチャーを認識する能力がある。
ヒトがあるものに注目するとき、それに対する手足の動作を起こす前に、目はすでにそれを見ている。ヒトの効果器のどれよりも、眼のほうがより早く位置情報を認知している。
注目視野は、20から30ビットを一度に把握できると言われる。これはアルファベットは5文字、ひらがなは4文字、漢字は2文字に相当する。英語の場合、一度に、15文字を読み進むという。先頭の1から7文字で意味を取り、次の8から15文字は周辺視野でみている、あるいは予測しているという
視覚的には連想記憶が働く。記憶は、神経細胞の結合パターンとして保持される。何度もキーボードを見て触っていると、神経細胞結合の発火の痕跡が残る。キーボードを見たという刺激だけで、あるキーがこの辺にあったよなという記憶が活性化される。
視覚は、いくつかの情報を同時に把握できるという能力がある。そのため、ヒトの抽象的な概念のうち、構造的な情報を認知できる。
ヒトは三次元の住民であるが、三次元に現象する物体を、紙などの二次元空間に表現したりもする。ヒトは、認知的に楽な情報を通して、実は高度な精神活動をしている。
ヒトは、視覚的な形状で、ほかの何かを象徴することも行ってきた。指ジェスチャー、地面に書いた絵、壁画、象形文字、表意文字、アイコン、ピクトグラムなどのしるしである。視覚的なしるしは、表現力が限定され、間接的である。
ベネチアのカーニバルでは、仮面をつけて着飾った人が練り歩く。その仮面は、美しくもあり、自然でない造作は不気味でもある。中国にも、バリ島など太平洋の島々にも、日本にも、仮面をつけるとか、濃厚な化粧をして、演舞することがある。これらは、自然な表情認知に対する刺激を、少し変えて、大きな効果をあげる。
音響的な言語情報は、時系列情報である。そして、音声は消え去る。短期記憶に入っても、資格のようには繰り返し再確認できない。そのため、ヒトは、聴覚からは、少ない量しか情報を把握できない。
体性感覚器は、外部を感知する触覚などと、内部の固有の情報を感知するものに、大別される。 手指を操作する支えとして、またセンサーとして、機能している。しかし、コンピュータの操作インターフェイスは、この身体性に対し無関心できた。
手指ができる運動は、握る(grip)、つまむ(pinch)｛指先(tip)、指腹(pulp)、側面(lateral)、ひっかけ(hook)、指間はさみ(finger)｝、ねじる(twist)、押す(push)、すくう(scoop)、と分類される。これらの組み合わせで、日常生活を送っている。
手指ができる動作の種類の豊かさに比べ、ヒトが現在のコンピュータを操作する際には、「押す」（クリック、キータイプ、タップ、スワイプなど）しか使っていない。
幼児は、まだ言葉になっていない喃語をしゃべる時期は、一人指さしが見られ、一語文をしゃべり始める時期には、他者を意識した伝達的指さしを行うという。
ヒトが、環境に対して何か効果を引き起こすとき、器用な手指だけを使っているのではない。ヒトは、馬に乗るときや、庭木剪定のため梯子に登るとき、カラダ全体を使い環境と会話する。
ヒトの言語表現のスピードは、経験的におおよそ、しゃべりなら160語/分、手書きは30語/分、タイプなら40語/分である。音声での言語表現は、習熟の必要がないばかりか、指よりも5倍速い。
視線は、社会的な行動の学習や進化に重要な役割がもつ。赤ちゃんが母を認知し、何を見ているかを認知する（＝他者の心の認識）。そして、あることに一緒に注目をすることを認知する（＝社会の始まり）。そして社会の中の個体存在を想像する（＝自己意識）。
哺乳類の中でも、人は特に大脳皮質が発達した。ヒトの大脳皮質の、感覚と運動に局所的な部分のかなりが、手指と発生器官周りに対応する。手指と発声は、ヒト知能の本質的な部分である。
機能局在な部分を除くと、大脳皮質の約2/3にも相当する広い領域が、連合野と呼ばれる。連合野は、高次な脳機能を具現化している皮質領域である。個々の感覚・運動機能よりも、それらを統合した部分が、ヒトのヒトたる部分である。マルチ・モダルが本来の自然なのである。
ヒトが一度に記憶できるのは4個までで、長期記憶からあるカテゴリーで想起できるのは3個までだそうである。現在のコンピュータのインターフェイス・デザインは、この事実を少しも考慮していない。
ヒトは、視覚的言語活動と、音響的言語活動とは、効率的に時分割で操れる。しかし、何か文章を考えているときに他人とおしゃべりはできない。どちらも、音響的言語活動である。ヒトは、こういうときに時分割を効率的にはできない。